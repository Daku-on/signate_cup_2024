{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Designationカラム"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_fullwidth_to_halfwidth_and_extract_invalid(\n",
    "    df: pd.DataFrame,\n",
    "    column_name: str,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    指定されたカラムに対して次の処理を行う:\n",
    "    1. 全角英字を半角英字に変換\n",
    "    2. それでもなおAからz以外の文字が含まれるユニークな値をリストとして返す\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): 対象のデータフレーム\n",
    "        column_name (str): 操作を行うカラムの名前\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: 修正後のデータフレーム\n",
    "        list: 条件に合わないレコードのユニークな値\n",
    "    \"\"\"\n",
    "    # 全角英字を半角英字に変換\n",
    "    def convert_fullwidth_to_halfwidth(text: str) -> str:\n",
    "        \"\"\"\n",
    "        全角英字を半角英字に変換するヘルパー関数\n",
    "\n",
    "        Args:\n",
    "            text (str): 入力文字列\n",
    "\n",
    "        Returns:\n",
    "            str: 半角に変換された文字列\n",
    "        \"\"\"\n",
    "        return \"\".join(\n",
    "            chr(ord(char) - 65248) if \"Ａ\" <= char <= \"Ｚ\" or \"ａ\" <= char <= \"ｚ\" else char\n",
    "            for char in text\n",
    "        )\n",
    "\n",
    "    # 対象カラムの全角英字を半角英字に変換\n",
    "    df[column_name] = df[column_name].apply(\n",
    "        lambda x: convert_fullwidth_to_halfwidth(x) if pd.notna(x) else x\n",
    "    )\n",
    "\n",
    "    # 半角スペースをアンダースコアに置換\n",
    "    df[column_name] = df[column_name].str.replace(\" \", \"_\", regex=False)\n",
    "    \n",
    "    replace_dict = {\n",
    "        \"𝙧\": \"r\",\n",
    "        \"α\": \"a\",\n",
    "        \"Տ\": \"S\",\n",
    "        \"ѵ\": \"v\",\n",
    "        \"×\": \"x\",\n",
    "        \"е\": \"e\",\n",
    "        \"Α\": \"A\",\n",
    "        \"А\": \"A\",\n",
    "        \"Μ\": \"M\",\n",
    "        \"Е\": \"E\",\n",
    "        \"Ѕ\": \"S\",\n",
    "    }\n",
    "    df = df.replace(\n",
    "        {column_name: replace_dict},\n",
    "        regex=True\n",
    "    )\n",
    "    \n",
    "    # Aからz以外の文字を含む行のindexを取得\n",
    "    invalid_indices = df[~df[column_name].str.match(r\"^[A-Za-z_]+$\", na=False)].index\n",
    "    \n",
    "    if len(invalid_indices) == 0:\n",
    "        # ここまでで前処理が完了していれば、カラムの値を小文字に変換して返す\n",
    "        df[column_name] = df[column_name].apply(\n",
    "            lambda x: x.lower() if pd.notna(x) else x\n",
    "        )\n",
    "        return df, []\n",
    "    else:\n",
    "        # 条件に合わないレコードのユニークな値をリストとして取得\n",
    "        print(\"there are invalid values in the column: {}\".format(column_name))\n",
    "        unique_invalid_values = df.loc[invalid_indices, column_name].unique().tolist()\n",
    "        return df, unique_invalid_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MonthlyIncomeカラム"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_and_convert_to_numeric(\n",
    "    df: pd.DataFrame, \n",
    "    column_name: str, \n",
    "    new_column_name: str\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    指定されたカラムから数字と「万」を抽出し、1万倍して新しいカラムに保存する。\n",
    "    正規表現にマッチしない場合、そのインデックスと値を記録する。\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): 入力データフレーム\n",
    "        column_name (str): 元のカラム名\n",
    "        new_column_name (str): 結果を保存する新しいカラム名\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: 処理結果が保存された新しいカラムが追加されたデータフレーム\n",
    "        list: 正規表現にマッチしなかったユニークな値のリスト\n",
    "    \"\"\"\n",
    "    unmatched_values = []\n",
    "    \n",
    "    def convert_to_number(\n",
    "        text: object, index: int\n",
    "    ) -> int:\n",
    "        if text is None or pd.isna(text):\n",
    "            return np.nan\n",
    "        text = str(text)\n",
    "        # 正規表現で「万」と数字を含む部分を抽出\n",
    "        match = re.search(r\"(\\d+(\\.\\d+)?)(万)?\", text)\n",
    "        if not match:\n",
    "            unmatched_values.append((index, text))\n",
    "            return None\n",
    "        number_str, _, unit = match.groups()\n",
    "        number = float(number_str)\n",
    "        if unit == \"万\":\n",
    "            number *= 10000\n",
    "        return int(number)\n",
    "\n",
    "    # 各レコードに対して処理を行い、新しいカラムに保存\n",
    "    df[new_column_name] = [\n",
    "        convert_to_number(value, idx) \n",
    "        for idx, value in enumerate(df[column_name])\n",
    "    ]\n",
    "    df[new_column_name] = df[new_column_name].astype(np.float32)\n",
    "    \n",
    "    if len(unmatched_values) == 0:\n",
    "        return df, []\n",
    "    else:\n",
    "        # 正規表現にマッチしなかったユニークな値をリストにして返す\n",
    "        print(\"there are unmatched values in the column: {}\".format(column_name))\n",
    "        unique_unmatched_values = list({value for _, value in unmatched_values})\n",
    "        print(unique_unmatched_values)\n",
    "    \n",
    "        return df, unique_unmatched_values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customer_infoカラム"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def customer_info_preprocess(\n",
    "    df: pd.DataFrame,  # 入力のデータフレーム\n",
    "    column_name: str,  # 処理対象のカラム名\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    各レコードに対して、指定されたカラムの文字列を処理し、\n",
    "    各単語を条件に基づいて新しいカラムに分類する。\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): 処理対象のデータフレーム\n",
    "        column_name (str): 対象カラム名\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: 処理結果を含むデータフレーム\n",
    "    \"\"\"\n",
    "    # 各レコードを処理\n",
    "    for index, row in df.iterrows():\n",
    "        # 句読点やコロン、改行などを半角スペースに変換\n",
    "        cleaned_text = re.sub(r\"[、。・：；,.;:?!/／\\n]\", \" \", str(row[column_name]))\n",
    "        \n",
    "        # 単語に分割\n",
    "        words = cleaned_text.split()\n",
    "\n",
    "        # 各単語に対して処理を実施\n",
    "        marriage_history = \" \".join([word for word in words if \"婚\" in word or \"独\" in word])\n",
    "        car = \" \".join([word for word in words if \"車\" in word])\n",
    "        children = \" \".join([word for word in words if \"婚\" not in word and \"独\" not in word and \"車\" not in word])\n",
    "\n",
    "        # 各レコードに新しいカラムを追加\n",
    "        df.at[index, \"marriage_history\"] = marriage_history\n",
    "        df.at[index, \"car\"] = car\n",
    "        df.at[index, \"children\"] = children\n",
    "    \n",
    "    # 各カラムの表記揺れを修正\n",
    "    def dict_replace_function(\n",
    "        text: str,\n",
    "        replace_dict: dict\n",
    "    ) -> str:\n",
    "        if text in replace_dict:\n",
    "            return str(replace_dict[text])\n",
    "        else:\n",
    "            raise ValueError(f\"'{text}' is not found in the replacement dictionary.\")\n",
    "\n",
    "    # car, childrenカラムの各レコードに対して置き換え処理を実施\n",
    "    # car辞書の作成\n",
    "    car_replace_dict = {\n",
    "    \"車未所持\": 0,\n",
    "    \"自動車未所有\": 0,\n",
    "    \"車保有なし\": 0,\n",
    "    \"乗用車なし\": 0,\n",
    "    \"自家用車なし\": 0,\n",
    "    \"車なし\": 0,\n",
    "    \"車あり\": 1,\n",
    "    \"車所持\": 1,\n",
    "    \"自家用車あり\": 1,\n",
    "    \"車保有\": 1,\n",
    "    \"乗用車所持\": 1,\n",
    "    \"自動車所有\": 1,\n",
    "    }\n",
    "    # children辞書の作成\n",
    "    children_replace_dict = {\n",
    "        \"子供なし\": 0,\n",
    "        \"子供無し\": 0,\n",
    "        \"無子\": 0,\n",
    "        \"子供ゼロ\": 0,\n",
    "        \"非育児家庭\": 0,\n",
    "        \"子育て状況不明\": np.nan,\n",
    "        \"子の数不詳\": np.nan,\n",
    "        \"子供の数不明\": np.nan,\n",
    "        \"こども1人\": 1,\n",
    "        \"1児\": 1,\n",
    "        \"子供1人\": 1,\n",
    "        \"子供有り(1人)\": 1,\n",
    "        \"子供有り 1人\": 1,\n",
    "        \"こども2人\": 2,\n",
    "        \"2児\": 2,\n",
    "        \"子供2人\": 2,\n",
    "        \"子供有り(2人)\": 2,\n",
    "        \"こども3人\": 3,\n",
    "        \"3児\": 3,\n",
    "        \"子供3人\": 3,\n",
    "        \"子供有り 2人\": 2,\n",
    "        \"子供有り 3人\": 3,\n",
    "        \"子供有り(3人)\": 3,\n",
    "        \"わからない\": np.nan,\n",
    "        \"不明\": np.nan,\n",
    "    }\n",
    "    \n",
    "    # データフレームの対象カラムに適用\n",
    "    df[\"car\"] = df[\"car\"].apply(dict_replace_function, replace_dict=car_replace_dict)\n",
    "    df[\"children\"] = df[\"children\"].apply(dict_replace_function, replace_dict=children_replace_dict)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_for_last_3_cols(\n",
    "    df: pd.DataFrame,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    データフレームに対して前処理を行う。\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): 前処理を行うデータフレーム\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: 前処理後のデータフレーム\n",
    "    \"\"\"\n",
    "    \n",
    "    # カラムごとの前処理\n",
    "    df, invalid_values = convert_fullwidth_to_halfwidth_and_extract_invalid(df, \"Designation\")\n",
    "    df, unmatched_values = extract_and_convert_to_numeric(df, \"MonthlyIncome\", \"MonthlyIncome_numeric\")\n",
    "    df = customer_info_preprocess(df, \"customer_info\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ローカルファイルを読み込む\n",
    "train_df = pd.read_csv(\"../data/train.csv\")\n",
    "test_df = pd.read_csv(\"../data/test.csv\")\n",
    "# google colaboratory で実行する場合は以下を有効にする\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# train_df = pd.read_csv(\"/content/drive/mydrive/signate_cup_2024_data/train.csv\")\n",
    "# test_df = pd.read_csv(\"/content/drive/mydrive/signate_cup_2024_data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = preprocess_for_last_3_cols(train_df)\n",
    "test_df = preprocess_for_last_3_cols(test_df)\n",
    "train_df.to_csv(\"../data/train_preprocessed.csv\", index=False)\n",
    "test_df.to_csv(\"../data/test_preprocessed.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "不要になったカラムは以下の2つ。デバッグ目的で残しているが、不要な場合は消してください。\n",
    "- MonthlyIncome\n",
    "- customer_info\n",
    "  \n",
    "新たにできたカラムは以下の4つ。名前が気に食わない場合は該当名を一括置換してください。\n",
    "- MonthlyIncome_numeric\n",
    "- marriage_history\n",
    "- car\n",
    "- children"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## おまけ。word2vecの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import normalize\n",
    "from typing import Optional\n",
    "\n",
    "def add_tfidf_features(\n",
    "    df: pd.DataFrame,\n",
    "    col_name: str,\n",
    "    max_features: int = 40,\n",
    "    n_components: Optional[int] = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    与えられたテキスト列をTF-IDFベクトルに変換し、SVDを使用して次元削減を行い、元のデータフレームに追加する関数。\n",
    "    n_componentsがNoneの場合、次元削減は行わない。\n",
    "\n",
    "    Args:\n",
    "        df: 入力データフレーム\n",
    "        col_name: TF-IDFベクトル化するテキスト列の名前\n",
    "        max_features: TF-IDFベクトル化時の最大特徴量数\n",
    "        n_components: SVDによる次元削減後の成分数。Noneの場合、次元削減を行わない。\n",
    "\n",
    "    Returns:\n",
    "        df: 変換後の特徴量が追加されたデータフレーム\n",
    "    \"\"\"\n",
    "\n",
    "    # TF-IDFベクトル化のためのVectorizerを初期化\n",
    "    vectorizer = TfidfVectorizer(max_features=max_features)\n",
    "    \n",
    "    # テキスト列をTF-IDFベクトルに変換\n",
    "    vectors = vectorizer.fit_transform(df[col_name])\n",
    "\n",
    "    if n_components is not None:\n",
    "        # SVDを使用して次元削減\n",
    "        svd = TruncatedSVD(n_components=n_components)\n",
    "        reduced_vectors = svd.fit_transform(vectors)\n",
    "        # ベクトルの正規化\n",
    "        normalized_vectors = normalize(reduced_vectors, norm=\"l2\")\n",
    "        # データフレームに変換\n",
    "        tfidf_df = pd.DataFrame(reduced_vectors, index=df.index)\n",
    "        # 新しいデータフレームの列名を設定（次元削減後）\n",
    "        cols = [(col_name + \"_svd_\" + str(f)) for f in range(tfidf_df.shape[1])]\n",
    "        tfidf_df.columns = cols\n",
    "    else:\n",
    "        # 次元削減を行わない場合、TF-IDFベクトルをデータフレームに変換\n",
    "        tfidf_df = pd.DataFrame(vectors.toarray(), index=df.index)\n",
    "        # 新しいデータフレームの列名を設定（次元削減なし）\n",
    "        cols = [(col_name + \"_tfidf_\" + str(f)) for f in range(tfidf_df.shape[1])]\n",
    "        tfidf_df.columns = cols\n",
    "    \n",
    "    # 変換された特徴量を元のデータに結合\n",
    "    df = pd.concat([df, tfidf_df], axis=\"columns\")\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_columns(\n",
    "    df: pd.DataFrame,\n",
    "    columns: list[str],\n",
    "    new_col_name: str,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    複数のカラムをアンダースコアで結合し、新しいカラムとして追加する関数\n",
    "\n",
    "    Args:\n",
    "        df: 入力データフレーム\n",
    "        columns: 結合するカラムのリスト\n",
    "        new_col_name: 新しいカラム名\n",
    "\n",
    "    Returns:\n",
    "        df: 新しいカラムが追加されたデータフレーム\n",
    "    \"\"\"\n",
    "    \n",
    "    # 複数のカラムをアンダースコアで結合\n",
    "    df[new_col_name] = df[columns].astype(str).agg('_'.join, axis=1)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"../data/train.csv\")\n",
    "test_df = pd.read_csv(\"../data/test.csv\")\n",
    "train_df = preprocess_for_last_3_cols(train_df)\n",
    "test_df = preprocess_for_last_3_cols(test_df)\n",
    "02\n",
    "train_df = concatenate_columns(train_df, [\"marriage_history\", \"car\", \"children\"], \"customer_info_concat\")\n",
    "test_df = concatenate_columns(test_df, [\"marriage_history\", \"car\", \"children\"], \"customer_info_concat\")\n",
    "train_df = add_tfidf_features(train_df, \"customer_info_concat\", max_features=40, n_components=3)\n",
    "test_df = add_tfidf_features(test_df, \"customer_info_concat\", max_features=40, n_components=None)\n",
    "train_df.to_csv(\"../data/train_preprocessed.csv\", index=False)\n",
    "test_df.to_csv(\"../data/test_preprocessed.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
